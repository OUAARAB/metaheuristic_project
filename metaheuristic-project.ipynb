{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10239648,"sourceType":"datasetVersion","datasetId":6332174},{"sourceId":10239771,"sourceType":"datasetVersion","datasetId":6332270},{"sourceId":10254000,"sourceType":"datasetVersion","datasetId":6342818},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T20:40:46.893466Z","iopub.execute_input":"2024-12-18T20:40:46.893872Z","iopub.status.idle":"2024-12-18T20:40:47.279864Z","shell.execute_reply.started":"2024-12-18T20:40:46.893840Z","shell.execute_reply":"2024-12-18T20:40:47.278800Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00001-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00003-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00002-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00007-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/README.md\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00008-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00005-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00006-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/.gitattributes\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.model\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00004-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/generation_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/transformers/transformers-4.42.0.dev0-py3-none-any.whl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-9b\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T20:40:54.109515Z","iopub.execute_input":"2024-12-18T20:40:54.109889Z","iopub.status.idle":"2024-12-18T20:40:55.841361Z","shell.execute_reply.started":"2024-12-18T20:40:54.109862Z","shell.execute_reply":"2024-12-18T20:40:55.840398Z"}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-9b/2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n    model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2', \n    load_in_8bit=False\n)\n # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 8, debug=False) -> Union[float, List[float]]:\n        \"\"\"\n        Optimized perplexity calculation using batching for efficiency.\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n        loss_list = []\n        \n        for i in range(0, len(input_texts), batch_size):\n            batch_texts = input_texts[i:i + batch_size]\n            with torch.no_grad():\n                model_inputs = self.tokenizer(\n                        batch_texts,\n                        return_tensors='pt',\n                        padding=True,\n                        truncation=True,\n                        max_length=512,  # Limit input sequence length\n                        add_special_tokens=True,\n                    ).to(DEVICE)\n\n                \n                \n                output = self.model(**model_inputs)\n                logits = output['logits']\n                \n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n                \n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n                \n                batch_losses = loss.view(len(batch_texts), -1).sum(dim=1) / model_inputs['attention_mask'].sum(dim=1)\n                loss_list.extend(batch_losses.cpu().tolist())\n                \n                if debug:\n                    print(f\"Processed batch {i // batch_size + 1} of {len(input_texts) // batch_size + 1}\")\n    \n        ppl = [exp(i) for i in loss_list]\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Enhanced memory clearing.\"\"\"\n        if torch.cuda.is_available():\n            del self.model\n            del self.tokenizer\n            gc.collect()\n            with torch.cuda.device(DEVICE):\n                torch.cuda.empty_cache()\n                torch.cuda.ipc_collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:43:35.790539Z","iopub.execute_input":"2024-12-21T17:43:35.790985Z","iopub.status.idle":"2024-12-21T17:43:38.612374Z","shell.execute_reply.started":"2024-12-21T17:43:35.790946Z","shell.execute_reply":"2024-12-21T17:43:38.611515Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport math\nfrom copy import deepcopy\n\n\nclass Particle:\n    def __init__(self, sequence, solution_df, scorer):\n        self.sequence = sequence[:]\n        self.velocity = [0] * len(sequence)\n        self.best_sequence = sequence[:]\n        self.best_perplexity = float('inf')\n        self.current_perplexity = float('inf')\n        self.scorer = scorer  # PerplexityCalculator instance\n        self.evaluate(solution_df)\n\n    def evaluate(self, solution_df):\n        submission_df = pd.DataFrame({'id': [solution_df['id'].iloc[0]], 'text': [' '.join(self.sequence)]})\n        self.current_perplexity = self.scorer.get_perplexity([' '.join(self.sequence)])[0]  # Using get_perplexity\n        if self.current_perplexity < self.best_perplexity:\n            self.best_perplexity = self.current_perplexity\n            self.best_sequence = self.sequence[:]\n\n\nclass Ant:\n    def __init__(self, sequence, solution_df, scorer):\n        self.sequence = sequence[:]\n        self.current_perplexity = float('inf')\n        self.best_perplexity = float('inf')\n        self.solution_df = solution_df\n        self.scorer = scorer\n        self.evaluate()\n\n    def evaluate(self):\n        submission_df = pd.DataFrame({'id': [self.solution_df['id'].iloc[0]], 'text': [' '.join(self.sequence)]})\n        self.current_perplexity = self.scorer.get_perplexity([' '.join(self.sequence)])[0]\n        if self.current_perplexity < self.best_perplexity:\n            self.best_perplexity = self.current_perplexity\n\ndef simulated_annealing(sequence, solution_df, scorer, initial_temp=100, cooling_rate=0.95, max_iterations=20):\n    \"\"\"SA comme recherche locale\"\"\"\n    current_sequence = sequence[:]\n    best_sequence = sequence[:]\n\n    submission_df = pd.DataFrame({'id': [solution_df['id'].iloc[0]], 'text': [' '.join(best_sequence)]})\n    best_perplexity = scorer.get_perplexity([' '.join(best_sequence)])[0]  # Using get_perplexity\n    current_temp = initial_temp\n\n    for _ in range(max_iterations):\n        # Générer un voisin par permutation\n        neighbor = current_sequence[:]\n        i, j = random.sample(range(len(neighbor)), 2)\n        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n\n        # Évaluer le voisin\n        neighbor_submission = pd.DataFrame({'id': [solution_df['id'].iloc[0]], 'text': [' '.join(neighbor)]})\n        neighbor_perplexity = scorer.get_perplexity([' '.join(neighbor)])[0]  # Using get_perplexity\n\n        # Accepter ou rejeter la solution\n        delta = neighbor_perplexity - best_perplexity\n        if delta < 0 or random.random() < math.exp(-delta / current_temp):\n            current_sequence = neighbor[:]\n            if neighbor_perplexity < best_perplexity:\n                best_sequence = neighbor[:]\n                best_perplexity = neighbor_perplexity\n\n        current_temp *= cooling_rate\n\n    return best_sequence, best_perplexity\n\ndef aco_with_sa(text, solution_df, scorer, n_ants=1, max_iterations=3, alpha=1.0, beta=2.0, evaporation_rate=0.5, pheromone_init=1.0):\n    \"\"\"ACO combiné avec SA\"\"\"\n    words = text.split()\n    n_words = len(words)\n\n    # Initialiser la matrice des phéromones\n    pheromones = np.full((n_words, n_words), pheromone_init)\n\n    # Initialisation des meilleurs résultats\n    global_best_sequence = None\n    global_best_perplexity = float('inf')\n\n    for iteration in range(max_iterations):\n        print(f\"Iteration {iteration + 1}/{max_iterations}, Best perplexity: {global_best_perplexity}\")\n\n        ants = []\n        for _ in range(n_ants):\n            # Construire une solution pour chaque fourmi\n            sequence = []\n            available_words = words[:]\n            for _ in range(n_words):\n                probabilities = []\n                for word in available_words:\n                    word_idx = words.index(word)\n                    probabilities.append(pheromones[len(sequence), word_idx] ** alpha)\n                probabilities = np.array(probabilities)\n                probabilities /= probabilities.sum()\n                chosen_word = random.choices(available_words, weights=probabilities, k=1)[0]\n                sequence.append(chosen_word)\n                available_words.remove(chosen_word)\n\n            # Créer une fourmi\n            ant = Ant(sequence, solution_df, scorer)\n            ants.append(ant)\n\n        # Recherche locale avec SA\n        for ant in ants:\n            improved_sequence, improved_perplexity = simulated_annealing(\n                ant.sequence,\n                solution_df,\n                scorer,\n                initial_temp=100,\n                cooling_rate=0.95,\n                max_iterations=10\n            )\n            if improved_perplexity < ant.best_perplexity:\n                ant.sequence = improved_sequence[:]\n                ant.best_perplexity = improved_perplexity\n\n            # Mettre à jour les meilleurs résultats globaux\n            if ant.best_perplexity < global_best_perplexity:\n                global_best_sequence = ant.sequence[:]\n                global_best_perplexity = ant.best_perplexity\n\n        # Mettre à jour les phéromones\n        pheromones *= (1 - evaporation_rate)\n        for ant in ants:\n            for i in range(len(ant.sequence) - 1):\n                word_idx1 = words.index(ant.sequence[i])\n                word_idx2 = words.index(ant.sequence[i + 1])\n                pheromones[word_idx1, word_idx2] += 1.0 / ant.best_perplexity\n\n    return global_best_sequence, global_best_perplexity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:43:42.814929Z","iopub.execute_input":"2024-12-21T17:43:42.815460Z","iopub.status.idle":"2024-12-21T17:43:42.833189Z","shell.execute_reply.started":"2024-12-21T17:43:42.815432Z","shell.execute_reply":"2024-12-21T17:43:42.832394Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def optimize_sequences(batch_size=5):\n    sample_submission = pd.read_csv(\"/kaggle/input/sample-submission/sample_submission.csv\")\n    results = []\n\n    # Divide data into batches\n    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n\n    for batch_idx in range(num_batches):\n        batch_start = batch_idx * batch_size\n        batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))\n        batch_data = sample_submission.iloc[batch_start:batch_end]\n\n        print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n\n        try:\n            scorer = PerplexityCalculator(\n                model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2', \n                load_in_8bit=False\n            )\n\n            for idx, row in batch_data.iterrows():\n                specific_solution = pd.DataFrame({'id': [row['id']], 'text': [row['text']]})\n                optimized_text, perplexity = aco_with_sa(\n                    row['text'],\n                    specific_solution,\n                    scorer,\n                    n_ants=10,\n                    max_iterations=10\n                )\n                print(f\"ID: {row['id']}, Final Perplexity: {perplexity}\")\n                results.append({'id': row['id'], 'text': optimized_text})\n\n            # Intermediate save\n            temp_df = pd.DataFrame(results)\n            temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n\n        except Exception as e:\n            print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n            # Handle batch errors gracefully\n            for idx, row in batch_data.iterrows():\n                results.append({'id': row['id'], 'text': row['text']})\n\n        # Clear GPU memory after each batch\n        if torch.cuda.is_available():\n            gc.collect()\n            with torch.cuda.device(DEVICE):\n                torch.cuda.empty_cache()\n                torch.cuda.ipc_collect()\n\n    # Final save\n    submission = pd.DataFrame(results)\n    submission.to_csv(\"submission.csv\", index=False)\n    return submission\n\n\nif __name__ == \"__main__\":\n        print(\"Starting optimization...\")\n        final_submission = optimize_sequences()\n        print(\"Optimization completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:12:01.625165Z","iopub.execute_input":"2024-12-21T18:12:01.625699Z","iopub.status.idle":"2024-12-21T19:02:34.790878Z","shell.execute_reply.started":"2024-12-21T18:12:01.625664Z","shell.execute_reply":"2024-12-21T19:02:34.790038Z"}},"outputs":[{"name":"stdout","text":"Starting optimization...\n\nProcessing batch 1/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb77d4305c724e238e973d1bd8ec8cc4"}},"metadata":{}},{"name":"stdout","text":"Iteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 497.56245599588635\nIteration 3/10, Best perplexity: 497.56245599588635\nIteration 4/10, Best perplexity: 497.56245599588635\nIteration 5/10, Best perplexity: 497.56245599588635\nIteration 6/10, Best perplexity: 488.2291821601048\nIteration 7/10, Best perplexity: 351.6909877849536\nIteration 8/10, Best perplexity: 351.6909877849536\nIteration 9/10, Best perplexity: 351.6909877849536\nIteration 10/10, Best perplexity: 351.6909877849536\nID: 0, Final Perplexity: 351.6909877849536\nIteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 1272.0827503862215\nIteration 3/10, Best perplexity: 1272.0827503862215\nIteration 4/10, Best perplexity: 1272.0827503862215\nIteration 5/10, Best perplexity: 1272.0827503862215\nIteration 6/10, Best perplexity: 1272.0827503862215\nIteration 7/10, Best perplexity: 1084.7002618019842\nIteration 8/10, Best perplexity: 1084.7002618019842\nIteration 9/10, Best perplexity: 1084.7002618019842\nIteration 10/10, Best perplexity: 1084.7002618019842\nID: 1, Final Perplexity: 1084.7002618019842\nIteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 855.242634693007\nIteration 3/10, Best perplexity: 771.1834866794766\nIteration 4/10, Best perplexity: 756.5397622253101\nIteration 5/10, Best perplexity: 756.5397622253101\nIteration 6/10, Best perplexity: 756.5397622253101\nIteration 7/10, Best perplexity: 756.5397622253101\nIteration 8/10, Best perplexity: 756.5397622253101\nIteration 9/10, Best perplexity: 756.5397622253101\nIteration 10/10, Best perplexity: 756.5397622253101\nID: 2, Final Perplexity: 756.5397622253101\nIteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 998.8706300801454\nIteration 3/10, Best perplexity: 998.8706300801454\nIteration 4/10, Best perplexity: 998.8706300801454\nIteration 5/10, Best perplexity: 998.8706300801454\nIteration 6/10, Best perplexity: 998.8706300801454\nIteration 7/10, Best perplexity: 998.8706300801454\nIteration 8/10, Best perplexity: 998.8706300801454\nIteration 9/10, Best perplexity: 998.8706300801454\nIteration 10/10, Best perplexity: 998.8706300801454\nID: 3, Final Perplexity: 998.8706300801454\nIteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 1035.3500584415167\nIteration 3/10, Best perplexity: 1035.3500584415167\nIteration 4/10, Best perplexity: 1035.3500584415167\nIteration 5/10, Best perplexity: 1035.3500584415167\nIteration 6/10, Best perplexity: 878.4924400712206\nIteration 7/10, Best perplexity: 878.4924400712206\nIteration 8/10, Best perplexity: 878.4924400712206\nIteration 9/10, Best perplexity: 878.4924400712206\nIteration 10/10, Best perplexity: 878.4924400712206\nID: 4, Final Perplexity: 878.4924400712206\n\nProcessing batch 2/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53981ef9a8594f7cb4b0823e0ab9d1df"}},"metadata":{}},{"name":"stdout","text":"Iteration 1/10, Best perplexity: inf\nIteration 2/10, Best perplexity: 723.9097493150708\nIteration 3/10, Best perplexity: 713.6122015598163\nIteration 4/10, Best perplexity: 707.6895578115276\nIteration 5/10, Best perplexity: 660.9927970947293\nIteration 6/10, Best perplexity: 660.9927970947293\nIteration 7/10, Best perplexity: 660.9927970947293\nIteration 8/10, Best perplexity: 660.9927970947293\nIteration 9/10, Best perplexity: 660.9927970947293\nIteration 10/10, Best perplexity: 660.9927970947293\nID: 5, Final Perplexity: 660.9927970947293\nOptimization completed!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"* The implementation combines Ant Colony Optimization (ACO) with Simulated Annealing (SA) to optimize sequences and minimize perplexity. ACO is used to construct candidate sequences based on pheromone trails that evolve with iterations, while SA refines individual solutions through a local search. \n* This hybrid approach leverages the exploratory power of ACO and the exploitative strength of SA. Observations include the processing of data in batches to handle large datasets, ensuring efficient optimization. \n* Each batch undergoes multiple iterations, with noticeable improvements in the \"Best perplexity\" metric for most IDs, indicating that the optimization is effectively reducing perplexity. \n* Final perplexities reveal lower values, showcasing enhanced model performance and suggesting the approach's success in generating better predictions. \n* The combination of ACO and SA allows for a robust balance between exploration and exploitation in the search space.","metadata":{}}]}